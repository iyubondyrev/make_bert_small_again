{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa6c9d7e-8f48-41e9-94ab-5c3b1b717694",
   "metadata": {},
   "source": [
    "# –ì–ª—É–±–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –§–ö–ù –í–®–≠\n",
    "## –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ 4: –£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –º–æ–¥–µ–ª–∏\n",
    "### –û—Ü–µ–Ω–∏–≤–∞–Ω–∏–µ –∏ —à—Ç—Ä–∞—Ñ—ã\n",
    "\n",
    "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –¥–æ–ø—É—Å—Ç–∏–º–∞—è –æ—Ü–µ–Ω–∫–∞ –∑–∞ —Ä–∞–±–æ—Ç—É ‚Äî __10 –±–∞–ª–ª–æ–≤__.\n",
    "\n",
    "–ó–∞–¥–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ. ¬´–ü–æ—Ö–æ–∂–∏–µ¬ª —Ä–µ—à–µ–Ω–∏—è —Å—á–∏—Ç–∞—é—Ç—Å—è –ø–ª–∞–≥–∏–∞—Ç–æ–º –∏ –≤—Å–µ –∑–∞–¥–µ–π—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç—É–¥–µ–Ω—Ç—ã (–≤ —Ç–æ–º —á–∏—Å–ª–µ —Ç–µ, —É –∫–æ–≥–æ —Å–ø–∏—Å–∞–ª–∏) –Ω–µ –º–æ–≥—É—Ç –ø–æ–ª—É—á–∏—Ç—å –∑–∞ –Ω–µ–≥–æ –±–æ–ª—å—à–µ 0 –±–∞–ª–ª–æ–≤. –í–µ—Å—å –∫–æ–¥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞–ø–∏—Å–∞–Ω —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ. –ß—É–∂–∏–º –∫–æ–¥–æ–º –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –∑–∞–ø—Ä–µ—â–∞–µ—Ç—Å—è –¥–∞–∂–µ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫. –í —Ä–∞–∑—É–º–Ω—ã—Ö —Ä–∞–º–∫–∞—Ö, –∫–æ–Ω–µ—á–Ω–æ. –í–∑—è—Ç—å –ø–∞—Ä—É –æ—á–µ–≤–∏–¥–Ω—ã—Ö —Å—Ç—Ä–æ—á–µ–∫ –∫–æ–¥–∞ –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–∞–∫–æ–≥–æ-—Ç–æ –Ω–µ–±–æ–ª—å—à–æ–≥–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞ –º–æ–∂–Ω–æ.\n",
    "\n",
    "–ù–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–¥–∞ –º–æ–∂–µ—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ –æ—Ç—Ä–∞–∑–∏—Ç—å—Å—è –Ω–∞ –æ—Ü–µ–Ω–∫–µ. –¢–∞–∫–∂–µ –æ—Ü–µ–Ω–∫–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–Ω–∏–∂–µ–Ω–∞ –∑–∞ –ø–ª–æ—Ö–æ —á–∏—Ç–∞–µ–º—ã–π –∫–æ–¥ –∏ –ø–ª–æ—Ö–æ –æ—Ñ–æ—Ä–º–ª–µ–Ω–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏. –í—Å–µ –æ—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞—Ç—å—Å—è –∫–æ–¥–æ–º –∏–ª–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏ –æ —Ç–æ–º, –∫–∞–∫ –æ–Ω–∏ –±—ã–ª–∏ –ø–æ–ª—É—á–µ–Ω—ã.\n",
    "\n",
    "__–ú—è–≥–∫–∏–π –¥–µ–¥–ª–∞–π–Ω 29.11.24 23:59__ \\\n",
    "__–ñ–µ—Å—Ç–∫–∏–π –¥–µ–¥–ª–∞–π–Ω 2.12.24 23:59__\n",
    "\n",
    "### –û –∑–∞–¥–∞–Ω–∏–∏\n",
    "\n",
    "–í —ç—Ç–æ–º –∑–∞–¥–∞–Ω–∏–∏ –≤–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç –Ω–∞—É—á–∏—Ç—å—Å—è —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á—É Named Entity Recognition (NER) –Ω–∞ —Å–∞–º–æ–º –ø–æ–ø—É–ª—è—Ä–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ ‚Äì [CoNLL-2003](https://paperswithcode.com/dataset/conll-2003). –í –≤–∞—à–µ–º —Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–∏ –±—É–¥–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π BERT, –∫–æ—Ç–æ—Ä—ã–π –≤–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–º–µ–Ω—å—à–∏—Ç—å —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–æ—Ç–µ—Ä—è–º–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –¥–æ —Ä–∞–∑–º–µ—Ä–∞ 20–ú –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –î–ª—è —ç—Ç–æ–≥–æ –≤—ã —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —Ä–µ–∞–ª–∏–∑—É–µ—Ç–µ —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é, —à–µ—Ä–∏–Ω–≥ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ —Ç–∞–∫ –¥–∞–ª–µ–µ.\n",
    "\n",
    "–í —ç—Ç–æ–º –∑–∞–¥–∞–Ω–∏–∏ –≤–∞–º –ø—Ä–∏–¥–µ—Ç—Å—è –ø—Ä–æ–≤–æ–¥–∏—Ç—å –¥–æ–≤–æ–ª—å–Ω–æ –º–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, –ø–æ—ç—Ç–æ–º—É –º—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –Ω–µ –ø–∏—Å–∞—Ç—å –≤–µ—Å—å –∫–æ–¥ –≤ —Ç–µ—Ç—Ä–∞–¥–∫–µ, –∞ –∑–∞–≤–µ—Å—Ç–∏ —Ä–∞–∑–Ω—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –±–ª–æ–∫–æ–≤ –∏ —Å–∫–æ–º–ø–æ–Ω–æ–≤–∞—Ç—å –≤—Å–µ –≤ –≤–∏–¥–µ –ø—Ä–æ–µ–∫—Ç–∞. –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç –≤–∞—à–µ–º—É –Ω–æ—É—Ç–±—É–∫—É –Ω–µ —Ä–∞–∑—Ä–∞—Å—Ç–∞—Ç—å—Å—è –∏ —Å–∏–ª—å–Ω–æ –æ–±–ª–µ–≥—á–∏—Ç –∑–∞–¥–∞—á—É –∏ –≤–∞–º, –∏ –ø—Ä–æ–≤–µ—Ä—è—é—â–∏–º. –¢–∞–∫ –∂–µ –ø–æ—Å—Ç–∞—Ä–∞–π—Ç–µ—Å—å –ª–æ–≥–≥–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –≤–∞—à–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –≤ wandb, —á—Ç–æ–±—ã –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ—Ç–µ—Ä—è–ª–æ—Å—å.\n",
    "\n",
    "### –û—Ü–µ–Ω–∏–≤–∞–Ω–∏–µ\n",
    "–û—Ü–µ–Ω–∫–∞ –∑–∞ —ç—Ç–æ –¥–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ –±—É–¥–µ—Ç —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å—Å—è –∏–∑ –æ—Ü–µ–Ω–∫–∏ –∑–∞ __–∑–∞–¥–∞–Ω–∏—è__ –∏ –∑–∞ __–æ—Ç—á–µ—Ç__, –≤ –∫–æ—Ç–æ—Ä–æ–º, –∫–∞–∫ –∏ —Ä–∞–Ω—å—à–µ, –æ—Ç –≤–∞—Å —Ç—Ä–µ–±—É–µ—Ç—Å—è –Ω–∞–ø–∏—Å–∞—Ç—å –æ –ø—Ä–æ–¥–µ–ª–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ. –ó–∞ –æ—Ç—á–µ—Ç –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –¥–æ 2-—Ö –±–∞–ª–ª–æ–≤, –æ–¥–Ω–∞–∫–æ –≤ —Å–ª—É—á–∞–µ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –æ—Ç—á–µ—Ç–∞ –º–æ–∂–Ω–æ –ø–æ—Ç–µ—Ä—è—Ç—å –±–∞–ª–ª—ã –∑–∞ —Å–∞–º–∏ –∑–∞–¥–∞–Ω–∏—è. –ó–∞–¥–∞–Ω–∏—è –¥–µ–ª—è—Ç—Å—è –Ω–∞ –¥–≤–µ —á–∞—Å—Ç–∏: _–Ω–æ–º–µ—Ä–Ω—ã–µ_ –∏ _–Ω–∞ –≤—ã–±–æ—Ä_. –ó–∞ _–Ω–æ–º–µ—Ä–Ω—ã–µ_ –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –≤ —Å—É–º–º–µ 6 –±–∞–ª–ª–æ–≤, –∑–∞ –∑–∞–¥–∞–Ω–∏—è _–Ω–∞ –≤—ã–±–æ—Ä_ –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –¥–æ 16. –¢–æ –µ—Å—Ç—å –∑–∞ –≤—Å–µ –¥–∑ –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å 24 –±–∞–ª–ª–∞. –í—Å–µ, —á—Ç–æ –≤—ã –Ω–∞–±–µ—Ä–µ—Ç–µ —Å–≤—ã—à–µ 10, –±—É–¥–µ—Ç —Å—á–∏—Ç–∞—Ç—å—Å—è –±–æ–Ω—É—Å–∞–º–∏.\n",
    "\n",
    "\n",
    "### –û –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
    "\n",
    "Named Entity Recognition ‚Äì¬†—ç—Ç–æ –∑–∞–¥–∞—á–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ –∫–ª–∞—Å—Å–∞–º —Å—É—â–Ω–æ—Å—Ç–µ–π. –í CoNLL-2003 –¥–ª—è –∏–º–µ–Ω–æ–≤–∞–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∞ **BIO** (Beggining, Inside, Outside), –≤ –∫–æ—Ç–æ—Ä–æ–π –º–µ—Ç–∫–∏ –æ–∑–Ω–∞—á–∞—é—Ç —Å–ª–µ–¥—É—é—â–µ–µ:\n",
    "\n",
    "- *B-{–º–µ—Ç–∫–∞}* ‚Äì –Ω–∞—á–∞–ª–æ —Å—É—â–Ω–æ—Å—Ç–∏ *{–º–µ—Ç–∫–∞}*\n",
    "- *I-{–º–µ—Ç–∫–∞}* ‚Äì –ø—Ä–æ–¥–æ–ª–∂–Ω–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏ *{–º–µ—Ç–∫–∞}*\n",
    "- *O* ‚Äì –Ω–µ —Å—É—â–Ω–æ—Å—Ç—å\n",
    "\n",
    "–°—É—â–µ—Å—Ç–≤—É—é—Ç —Ç–∞–∫ –∂–µ –∏ –¥—Ä—É–≥–∏–µ —Å–ø–æ—Å–æ–±—ã –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, BILUO. –ü–æ—á–∏—Ç–∞—Ç—å –æ –Ω–∏—Ö –º–æ–∂–Ω–æ [—Ç—É—Ç](https://en.wikipedia.org/wiki/Inside‚Äìoutside‚Äìbeginning_(tagging)) –∏ [—Ç—É—Ç](https://www.youtube.com/watch?v=dQw4w9WgXcQ).\n",
    "\n",
    "–í—Å–µ–≥–æ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ –µ—Å—Ç—å 9 —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç–æ–∫.\n",
    "- O ‚Äì¬†—Å–ª–æ–≤—É –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –Ω–∏ –æ–¥–Ω–∞ —Å—É—â–Ω–æ—Å—Ç—å.\n",
    "- B-PER/I-PER ‚Äì¬†—Å–ª–æ–≤–æ –∏–ª–∏ –Ω–∞–±–æ—Ä —Å–ª–æ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç—É–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º—É _—á–µ–ª–æ–≤–µ–∫—É_.\n",
    "- B-ORG/I-ORG ‚Äì¬†—Å–ª–æ–≤–æ –∏–ª–∏ –Ω–∞–±–æ—Ä —Å–ª–æ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç—É–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π _–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏_.\n",
    "- B-LOC/I-LOC ‚Äì¬†—Å–ª–æ–≤–æ –∏–ª–∏ –Ω–∞–±–æ—Ä —Å–ª–æ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç—É–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π _–ª–æ–∫–∞—Ü–∏–∏_.\n",
    "- B-MISC/I-MISC ‚Äì¬†—Å–ª–æ–≤–æ –∏–ª–∏ –Ω–∞–±–æ—Ä —Å–ª–æ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç—É–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –Ω–∏ –∫ –æ–¥–Ω–æ–π –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö. –ù–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å, –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –∏—Å–∫—É—Å—Å—Ç–≤–∞, –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ –∏ —Ç.–¥.\n",
    "\n",
    "–ü—Ä–∏—Å—Ç—É–ø–∏–º!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac32f2c-fa53-4f8b-a400-ad8b5f8acebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils import set_all_seeds\n",
    "\n",
    "\n",
    "set_all_seeds(1488)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe56a70-a72d-40a0-9ae3-395ec6460657",
   "metadata": {},
   "source": [
    "–ù–∞—á–Ω–µ–º —Å –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ef87648-86ab-4f81-9db3-5cb7f54c575d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"eriktks/conll2003\")\n",
    "\n",
    "dataset = dataset.remove_columns([\"id\", \"pos_tags\", \"chunk_tags\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7c1a5b0-6ca1-4159-9ce6-cff88aca6b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b0e34d-edca-40bc-83ac-cff0c2872f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "958cdff9-6ea1-4f7f-808b-dbe5620c27e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU\tB-ORG\n",
      "rejects\tO\n",
      "German\tB-MISC\n",
      "call\tO\n",
      "to\tO\n",
      "boycott\tO\n",
      "British\tB-MISC\n",
      "lamb\tO\n",
      ".\tO\n"
     ]
    }
   ],
   "source": [
    "words = dataset[\"train\"][0][\"tokens\"]\n",
    "labels = dataset[\"train\"][0][\"ner_tags\"]\n",
    "\n",
    "for i in range(len(words)):\n",
    "    print(f'{words[i]}\\t{label_names[labels[i]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef2312e-1335-4afa-a6e6-3cdde8515fe5",
   "metadata": {},
   "source": [
    "### –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞\n",
    "\n",
    "–ù–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –≤—Å–µ–≥–æ –¥–æ–º–∞—à–Ω–µ–≥–æ –∑–∞–¥–∞–Ω–∏—è –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å _cased_ –≤–µ—Ä—Å–∏—é BERT, —Ç–æ –µ—Å—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –±—É–¥–µ—Ç —É—á–∏—Ç—ã–≤–∞—Ç—å —Ä–µ–≥–∏—Å—Ç—Ä —Å–ª–æ–≤. –î–ª—è –∑–∞–¥–∞—á–∏ NER —Ä–µ–≥–∏—Å—Ç—Ä –≤–∞–∂–µ–Ω, —Ç–∞–∫ –∫–∞–∫ –∏–º–µ–Ω–∞ –∏ –Ω–∞–∑–≤–∞–Ω–∏—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –∏–ª–∏ –ø—Ä–µ–¥–º–µ—Ç–æ–≤ –∏—Å–∫—É—Å—Å—Ç–≤–∞ —á–∞—Å—Ç–æ –ø–∏—à—É—Ç—Å—è —Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã, –∏ –±—É–¥–µ—Ç –≥–ª—É–ø–æ –ø—Ä—è—Ç–∞—Ç—å –æ—Ç –º–æ–¥–µ–ª–∏ —Ç–∞–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05edcd4e-5360-41a8-b403-a9084d6a3a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, PreTrainedTokenizerBase\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f64076-829a-49f1-af58-6fe60c66f965",
   "metadata": {},
   "source": [
    "–ü—Ä–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–≤–∞ –º–æ–≥—É—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç—å—Å—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–æ–≤ (–∫–∞–∫ —Å–ª–æ–≤–æ `Fischler` –∏–∑ –ø—Ä–∏–º–µ—Ä–∞ –Ω–∏–∂–µ), –∏–∑-–∑–∞ —á–µ–≥–æ –ø–æ—è–≤–∏—Ç—Å—è –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É —á–∏—Å–ª–æ–º —Ç–æ–∫–µ–Ω–æ–≤ –∏ –º–µ—Ç–æ–∫. –≠—Ç–æ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–∞–º –ø—Ä–∏–¥–µ—Ç—Å—è —É—Å—Ç—Ä–∞–Ω–∏—Ç—å –≤—Ä—É—á–Ω—É—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ebc8789-0bba-4c96-aa1a-84403c93260e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–ª–æ–≤–∞:  ['Only', 'France', 'and', 'Britain', 'backed', 'Fischler', \"'s\", 'proposal', '.']\n",
      "–¢–æ–∫–µ–Ω—ã: ['[CLS]', 'Only', 'France', 'and', 'Britain', 'backed', 'Fi', '##sch', '##ler', \"'\", 's', 'proposal', '.', '[SEP]']\n",
      "–ú–µ—Ç–∫–∏: ['O', 'B-LOC', 'O', 'B-LOC', 'O', 'B-PER', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "example = dataset[\"train\"][12]\n",
    "words = example[\"tokens\"]\n",
    "tags = [label_names[t] for t in example[\"ner_tags\"]]\n",
    "tokenized_text = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "\n",
    "print('–°–ª–æ–≤–∞: ', words)\n",
    "print('–¢–æ–∫–µ–Ω—ã:', tokenized_text.tokens())\n",
    "print('–ú–µ—Ç–∫–∏:', tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34949bff-d7e9-47b3-aea7-82f1332a899c",
   "metadata": {},
   "source": [
    "__–ó–∞–¥–∞–Ω–∏–µ 1 (1 –±–∞–ª–ª).__ –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–π—Ç–µ –≤–µ—Å—å –¥–∞—Ç–∞—Å–µ—Ç –∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤—ã—Ä–∞–≤–Ω–∏—Ç–µ —Ç–æ–∫–µ–Ω—ã —Å –º–µ—Ç–∫–∞–º–∏ —Ç–∞–∫, —á—Ç–æ–±—ã –∫–∞–∂–¥–æ–º—É —Ç–æ–∫–µ–Ω—É —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª–∞ –æ–¥–Ω–∞ –º–µ—Ç–∫–∞. –ü—Ä–∏ —ç—Ç–æ–º –≤–∞–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–æ—Ç–∞—Ü–∏—é BIO. –ò –Ω–µ –∑–∞–±—É–¥—å—Ç–µ –ø—Ä–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã! –î–æ–ª–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å—Å—è —á—Ç–æ-—Ç–æ —Ç–∞–∫–æ–µ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffd9191b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100, 0, 5, 0, 5, 0, 1, 2, 2, 0, 0, 0, 0, -100]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def align_labels_with_tokens(ner_tags: list[int], tokenized_text) -> list[int]:\n",
    "    aligned_labels = []\n",
    "    current_word_idx = -1\n",
    "    prev_word_id = None\n",
    "\n",
    "    for word_id in tokenized_text.word_ids():\n",
    "        if word_id is None:\n",
    "            aligned_labels.append(-100)\n",
    "        elif word_id != prev_word_id:\n",
    "            current_word_idx += 1\n",
    "            aligned_labels.append(ner_tags[current_word_idx])\n",
    "        else:\n",
    "            label = ner_tags[current_word_idx]\n",
    "            aligned_labels.append(label + 1 if label % 2 == 1 else label)\n",
    "\n",
    "        prev_word_id = word_id\n",
    "\n",
    "    assert len(tokenized_text.tokens()) == len(aligned_labels)\n",
    "    \n",
    "    return aligned_labels\n",
    "\n",
    "\n",
    "align_labels_with_tokens(example[\"ner_tags\"], tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8352b8f-f60a-4844-b428-9e866678dc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í—ã—Ä–æ–≤–Ω–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏: [-100, 0, 5, 0, 5, 0, 1, 2, 2, 0, 0, 0, 0, -100]\n",
      "–í—ã—Ä–æ–≤–Ω–µ–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –º–µ—Ç–æ–∫: [-100, 'O', 'B-LOC', 'O', 'B-LOC', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', -100]\n"
     ]
    }
   ],
   "source": [
    "aligned_labels = align_labels_with_tokens(example[\"ner_tags\"], tokenized_text)\n",
    "tags = [label_names[t] if t > -1 else t for t in aligned_labels]\n",
    "print(\"–í—ã—Ä–æ–≤–Ω–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏:\", aligned_labels)\n",
    "print(\"–í—ã—Ä–æ–≤–Ω–µ–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –º–µ—Ç–æ–∫:\", tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "622983fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2809, 1699, 1105, 2855, 5534, 17355, 9022, 2879, 112, 188, 5835, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, 5, 0, 5, 0, 1, 2, 2, 0, 0, 0, 0, -100]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    aligned_labels = align_labels_with_tokens(example[\"ner_tags\"], tokenized_inputs)\n",
    "    tokenized_inputs[\"labels\"] = aligned_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenize_and_align_labels(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14762046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=False)\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e9399-8393-4312-88aa-53b727d9df7d",
   "metadata": {},
   "source": [
    "### –ú–µ—Ç—Ä–∏–∫–∞\n",
    "\n",
    "–î–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ NER –æ–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç F1 –º–µ—Ä—É —Å –º–∏–∫—Ä–æ-—É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ–º. –ú—ã –∑–∞–≥—Ä—É–∑–∏–º –µ–µ –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ `seqeval`. –§—É–Ω–∫—Ü–∏—è `f1_score` –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –¥–≤–∞ 2d —Å–ø–∏—Å–∫–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏, –∑–∞–ø–∏—Å–∞–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–æ–º, –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–ª—è –Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–µ F1. –í—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–µ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3df54ab-c65b-40e0-b479-25d6f29e5f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in /opt/conda/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.26.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.5.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "380833ce-1b8e-4b00-90ee-9126df16c19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc901ecf-2de9-4c3f-862c-cf78871d8d9f",
   "metadata": {},
   "source": [
    "–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å –ø–æ–¥—Å—á–µ—Ç–∞ F1 –¥–ª—è NER –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –º–æ–≥—É—Ç –∑–∞—Å—á–∏—Ç—ã–≤–∞—Ç—å—Å—è –∫–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ. –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–ª–∞ `['I-PER', 'I-PER']`, —Ç–æ –º—ã –º–æ–∂–µ–º –¥–æ–≥–∞–¥–∞—Ç—å—Å—è, —á—Ç–æ –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å `['B-PER', 'I-PER']`, —Ç–∞–∫ –∫–∞–∫ —Å—É—â–Ω–æ—Å—Ç—å –Ω–µ –º–æ–∂–µ—Ç –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å `I-`. –§—É–Ω–∫—Ü–∏—è `f1_score` —É—á–∏—Ç—ã–≤–∞–µ—Ç —ç—Ç–æ –∏ –ø–æ—ç—Ç–æ–º—É —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º–∏ –º–µ—Ç–æ–∫."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61400bf-712a-4dfb-a08f-326c5db10eb2",
   "metadata": {},
   "source": [
    "### –ú–æ–¥–µ–ª—å\n",
    "\n",
    "–í –∫–∞—á–µ—Å—Ç–≤–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –º—ã –≤–æ–∑—å–º–µ–º `bert-base-cased`. –ö–∞–∫ –≤—ã –ø–æ–Ω–∏–º–∞–µ—Ç–µ, –æ–Ω –Ω–µ –æ–±—É—á–∞–ª—Å—è –Ω–∞ –∑–∞–¥–∞—á—É NER. –ü–æ—ç—Ç–æ–º—É –ø—Ä–µ–∂–¥–µ —á–µ–º –ø—Ä–∏—Å—Ç—É–ø–∞—Ç—å –∫ —É–º–µ–Ω—å—à–µ–Ω–∏—é —Ä–∞–∑–º–µ—Ä–∞ BERT, –µ–≥–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–æ–æ–±—É—á–∏—Ç—å.\n",
    "\n",
    "__–ó–∞–¥–∞–Ω–∏–µ 2 (1 –±–∞–ª–ª)__ –î–æ–æ–±—É—á–∏—Ç–µ `bert-base-cased` –Ω–∞ –Ω–∞—à–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ —Å –ø–æ–º–æ—â—å—é –æ–±—ã—á–Ω–æ–≥–æ fine-tuning. –£ –≤–∞—Å –¥–æ–ª–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å—Å—è —Ö–æ—Ç—è –±—ã 0.9 F1 –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ. –ó–∞–º–µ—Ç—å—Ç–µ, —á—Ç–æ —á–µ–º –≤—ã—à–µ –∫–∞—á–µ—Å—Ç–≤–æ –±–æ–ª—å—à–æ–π –º–æ–¥–µ–ª–∏, —Ç–µ–º –ª—É—á—à–µ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —É—á–µ–Ω–∏–∫. –î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `Trainer` –∏–∑ Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a69c130c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c27afcff4f4e58bb639c7abaf0ea12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_10545/3710745720.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 11:59, Epoch 18/19]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.017400</td>\n",
       "      <td>0.059140</td>\n",
       "      <td>0.934718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.070830</td>\n",
       "      <td>0.942783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.082287</td>\n",
       "      <td>0.943296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.085419</td>\n",
       "      <td>0.945831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=0.026098366398364305, metrics={'train_runtime': 719.8543, 'train_samples_per_second': 355.628, 'train_steps_per_second': 2.778, 'total_flos': 8411326834240548.0, 'train_loss': 0.026098366398364305, 'epoch': 18.181818181818183})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from dataset import FilteredDataCollatorForTokenClassification, get_tokenized_dataset\n",
    "from metrics import compute_f1, compute_f1_on_dataset\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerBase\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from utils import set_all_seeds\n",
    "\n",
    "set_all_seeds(1488)\n",
    "\n",
    "label_names = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(label_names))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "tokenized_dataset = get_tokenized_dataset()\n",
    "\n",
    "data_collator = FilteredDataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_vanilla\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    learning_rate=4e-5,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    max_steps=2000,\n",
    "    weight_decay=0.03,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_f1,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56f89ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 216/216 [00:02<00:00, 90.25it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.901572409000087"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_f1_on_dataset(model, tokenized_dataset[\"test\"], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cda5a7-7fb9-43bc-84e3-e66ec6a48d91",
   "metadata": {},
   "source": [
    "### –§–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "\n",
    "–ú–æ–∂–Ω–æ –∑–∞–º–µ—Ç–∏—Ç—å, —á—Ç–æ –Ω–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç –º–∞—Ç—Ä–∏—Ü–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–Ω–∏–º–∞–µ—Ç $V \\cdot H = 28996 \\cdot 768 = 22.268.928$ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –≠—Ç–æ a–∂ –ø—è—Ç–∞—è —á–∞—Å—Ç—å –æ—Ç –≤—Å–µ–π –º–æ–¥–µ–ª–∏! –î–∞–≤–∞–π—Ç–µ –ø–æ–ø—Ä–æ–±—É–µ–º —á—Ç–æ-—Ç–æ —Å —ç—Ç–∏–º —Å–¥–µ–ª–∞—Ç—å. –í –º–æ–¥–µ–ª–∏ [ALBERT](https://arxiv.org/pdf/1909.11942.pdf) –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ñ–∞–∫—Ç–æ—Ä–∏–∑–æ–≤–∞—Ç—å –º–∞—Ç—Ä–∏—Ü—É —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –¥–≤—É—Ö –Ω–µ–±–æ–ª—å—à–∏—Ö –º–∞—Ç—Ä–∏—Ü. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –±—É–¥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å $V \\cdot E + E \\cdot H$ —ç–ª–µ–º–µ–Ω—Ç–æ–≤, —á—Ç–æ –≥–æ—Ä–∞–∑–¥–æ –º–µ–Ω—å—à–µ $V \\cdot H$, –µ—Å–ª–∏ $H \\gg E$. –ê–≤—Ç–æ—Ä—ã –≤—ã–±–∏—Ä–∞—é—Ç $E = 128$, –æ–¥–Ω–∞–∫–æ –Ω–∏—á–µ–≥–æ –Ω–µ –º–µ—à–∞–µ—Ç –Ω–∞–º –≤–∑—è—Ç—å –ª—é–±–æ–µ –¥—Ä—É–≥–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤—ã–±—Ä–∞–≤ $H = 64$, –º—ã —É–º–µ–Ω—å—à–∏–º —á–∏—Å–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—Ä–∏–º–µ—Ä–Ω–æ –Ω–∞ 20–ú.\n",
    "\n",
    "__–ó–∞–¥–∞–Ω–∏–µ 3 (1 –±–∞–ª–ª).__ –ù–∞–ø–∏—à–∏—Ç–µ –∫–ª–∞—Å—Å-–æ–±–µ—Ä—Ç–∫—É –Ω–∞–¥ —Å–ª–æ–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ–∞–ª–∏–∑—É–µ—Ç —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –Ω–∞ –¥–≤–µ –º–∞—Ç—Ä–∏—Ü—ã, –∏ –¥–æ–æ–±—É—á–∏—Ç–µ —Ñ–∞–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å. –ó–∞–º–µ—Ç—å—Ç–µ, –æ–±–µ –º–∞—Ç—Ä–∏—Ü—ã –º–æ–∂–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å –ø–æ–º–æ—â—å—é SVD —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è, —á—Ç–æ–±—ã –Ω–∞—á–∞–ª—å–Ω–æ–µ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ –±—ã–ª–æ —Ö–æ—Ä–æ—à–∏–º. –≠—Ç–æ —Å—ç–∫–æ–Ω–æ–º–∏—Ç –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –¥–æ–æ–±—É—á–µ–Ω–∏–∏. –° —Ä–∞–Ω–≥–æ–º —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è $H = 64$ —É –≤–∞—Å –¥–æ–ª–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å—Å—è F1 –±–æ–ª—å—à–µ 0.87."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4542bc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_11616/213357216.py:44: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 08:51, Epoch 13/14]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.100135</td>\n",
       "      <td>0.917765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.111406</td>\n",
       "      <td>0.927289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.112471</td>\n",
       "      <td>0.927940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=0.021571135704716048, metrics={'train_runtime': 532.1133, 'train_samples_per_second': 360.825, 'train_steps_per_second': 2.819, 'total_flos': 6443397903166284.0, 'train_loss': 0.021571135704716048, 'epoch': 13.636363636363637})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from factorized_embeddings import initialize_with_svd\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from dataset import FilteredDataCollatorForTokenClassification, get_tokenized_dataset\n",
    "from metrics import compute_f1, compute_f1_on_dataset\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerBase\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from utils import set_all_seeds\n",
    "\n",
    "set_all_seeds(1488)\n",
    "\n",
    "model_with_factorized_embeds = AutoModelForTokenClassification.from_pretrained('results_vanilla/checkpoint-2000', num_labels=len(label_names))\n",
    "\n",
    "\n",
    "hidden_dim = 64\n",
    "original_embedding = model_with_factorized_embeds.bert.embeddings.word_embeddings\n",
    "factorized_embedding = initialize_with_svd(original_embedding, hidden_dim)\n",
    "\n",
    "model_with_factorized_embeds.bert.embeddings.word_embeddings = factorized_embedding\n",
    "\n",
    "label_names = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "tokenized_dataset = get_tokenized_dataset()\n",
    "\n",
    "data_collator = FilteredDataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results-new-embeds\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    learning_rate=4e-5,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=64,\n",
    "    max_steps=1500,\n",
    "    weight_decay=0.03,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_with_factorized_embeds,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_f1\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d303e533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 216/216 [00:02<00:00, 89.30it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8820092439173279"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_f1_on_dataset(model_with_factorized_embeds, tokenized_dataset[\"test\"], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6220bd15-3681-4006-b7e0-44838b3500ad",
   "metadata": {},
   "source": [
    "### –î–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –∑–Ω–∞–Ω–∏–π\n",
    "\n",
    "–î–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –∑–Ω–∞–Ω–∏–π ‚Äì¬†—ç—Ç–æ –ø–∞—Ä–∞–¥–∏–≥–º–∞ –æ–±—É—á–µ–Ω–∏—è, –≤ –∫–æ—Ç–æ—Ä–æ–π –∑–Ω–∞–Ω–∏—è –º–æ–¥–µ–ª–∏-—É—á–∏—Ç–µ–ª—è –¥–∏—Å—Ç–∏–ª–ª–∏—Ä—É—é—Ç—Å—è –≤ –º–æ–¥–µ–ª—å-—É—á–µ–Ω–∏–∫–∞. –£—á–µ–Ω–∏–∫–æ–º –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –º–µ–Ω—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞, —Ä–µ—à–∞—é—â–∞—è —Ç—É –∂–µ –∑–∞–¥–∞—á—É, –æ–¥–Ω–∞–∫–æ –æ–±—ã—á–Ω–æ —É—á–µ–Ω–∏–∫ –∏–º–µ–µ—Ç —Ç—É –∂–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, —á—Ç–æ –∏ —É—á–∏—Ç–µ–ª—å. –ü—Ä–∏ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–≤–∞ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞ –æ—à–∏–±–∫–∏:\n",
    "\n",
    "1. –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏—è.\n",
    "1. –§—É–Ω–∫—Ü–∏—è, –∑–∞–¥–∞—é—â–∞—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —É—á–∏—Ç–µ–ª—è –∏ —É—á–µ–Ω–∏–∫–∞. –ß–∞—â–µ –≤—Å–µ–≥–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç KL-–¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—é.\n",
    "\n",
    "–î–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —É—á–∏—Ç–µ–ª—è –Ω–µ –±—ã–ª–æ –≤—ã—Ä–æ–∂–¥–µ–Ω–Ω—ã–º, –∫ softmax –¥–æ–±–∞–≤–ª—è—é—Ç —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –±–æ–ª—å—à–µ 1, –Ω–∞–ø—Ä–∏–º–µ—Ä, 2 –∏–ª–∏ 5.   \n",
    "__–í–∞–∂–Ω–æ:__ –ø—Ä–∏ –¥–µ–ª–µ–Ω–∏–∏ –ª–æ–≥–∏—Ç–æ–≤ –Ω–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –∑–Ω–∞—á–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ —É–º–µ–Ω—å—à–∞—é—Ç—Å—è –≤ $\\tau^2$ —Ä–∞–∑ (–ø—Ä–æ–≤–µ—Ä—å—Ç–µ —ç—Ç–æ!). –ü–æ—ç—Ç–æ–º—É –¥–ª—è –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏—è –∏—Ö –≤ –∏–∑–Ω–∞—á–∞–ª—å–Ω—ã–π –º–∞—Å—à—Ç–∞–± –æ—à–∏–±–∫—É –Ω–∞–¥–æ –¥–æ–º–Ω–æ–∂–∏—Ç—å –Ω–∞ $\\tau^2$. –ü–æ–¥—Ä–æ–±–Ω–µ–µ –æ–± —ç—Ç–æ–º –º–æ–∂–Ω–æ –ø–æ—á–∏—Ç–∞—Ç—å –≤ —Ä–∞–∑–¥–µ–ª–µ 2.1 [–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–∏](https://arxiv.org/pdf/1503.02531).\n",
    "\n",
    "<img src=\"https://intellabs.github.io/distiller/imgs/knowledge_distillation.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e921f-279e-46ae-8c70-5d715b91106e",
   "metadata": {},
   "source": [
    "__–ó–∞–¥–∞–Ω–∏–µ 4 (3 –±–∞–ª–ª–∞).__ –†–µ–∞–ª–∏–∑—É–π—Ç–µ –º–µ—Ç–æ–¥ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–Ω—ã–π –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ. –î–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –æ—à–∏–±–∫–∏ –º–µ–∂–¥—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏ —É—á–µ–Ω–∏–∫–∞ –∏ —É—á–∏—Ç–µ–ª—è –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ KL-–¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—é [`nn.KLDivLoss(reduction=\"batchmean\")`](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html) (–æ–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –≤–æ—Ä–º–∞—Ç –µ–µ –≤—Ö–æ–¥–æ–≤). –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏—Ç–æ–≥–æ–≤–æ–π –æ—à–∏–±–∫–∏ —Å—É–º–º–∏—Ä—É–π—Ç–µ –º—è–≥–∫—É—é –æ—à–∏–±–∫—É —Å –∂–µ—Å—Ç–∫–æ–π.   \n",
    "–í –∫–∞—á–µ—Å—Ç–≤–µ —É—á–∏—Ç–µ–ª—è –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–æ–æ–±—É—á–µ–Ω–Ω—ã–π BERT –∏–∑ –∑–∞–¥–∞–Ω–∏—è 2. –í –∫–∞—á–µ—Å—Ç–≤–µ —É—á–µ–Ω–∏–∫–∞ –≤–æ–∑—å–º–∏—Ç–µ –Ω–µ–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å —Å —Ä–∞–∑–º–µ—Ä–æ–º __–Ω–µ –±–æ–ª—å—à–µ 20M__ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –í—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –º–∞—Ç—Ä–∏—Ü—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —á–∏—Å–ª–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ï—Å–ª–∏ –≤—ã –≤—Å–µ —Å–¥–µ–ª–∞–ª–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ, —Ç–æ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –≤—ã –¥–æ–ª–∂–Ω—ã –ø–æ–ª—É—á–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ F1 –Ω–µ –º–µ–Ω—å—à–µ 0.7. –í–∞–º –¥–æ–ª–∂–Ω–æ —Ö–≤–∞—Ç–∏—Ç—å –ø—Ä–∏–º–µ—Ä–Ω–æ 20–∫ –∏—Ç–µ—Ä–∞—Ü–∏–π –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —ç—Ç–æ–≥–æ. –ï—Å–ª–∏ —É –≤–∞—Å —á—Ç–æ-—Ç–æ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è, —Ç–æ –º–æ–∂–Ω–æ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ —Å—Ç–∞—Ç—å—é –ø—Ä–æ [DistilBERT](https://arxiv.org/abs/1910.01108) –∏ –Ω–∞ [—ç—Ç—É —Å—Ç–∞—Ç—å—é](https://www.researchgate.net/publication/375758425_Knowledge_Distillation_Scheme_for_Named_Entity_Recognition_Model_Based_on_BERT).\n",
    "\n",
    "__–í–∞–∂–Ω–æ:__\n",
    "* –ù–µ –∑–∞–±—ã–≤–∞–π—Ç–µ –¥–æ–±–∞–≤–ª—è—Ç—å _warmup_ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —É—á–µ–Ω–∏–∫–∞.\n",
    "* –ù–µ –∑–∞–±—ã–≤–∞–π—Ç–µ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å —É—á–∏—Ç–µ–ª—è –≤ —Ä–µ–∂–∏–º _eval_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e996db95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 19232137\n",
      "Trainable Parameters: 19232137\n",
      "Non-Trainable Parameters: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/make_bert_small_again/knowledge_distillation.py:27: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `DistillationTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8000' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8000/8000 10:30, Epoch 72/73]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>12.419000</td>\n",
       "      <td>21.333111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>5.506700</td>\n",
       "      <td>20.105515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.864900</td>\n",
       "      <td>19.280529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.829600</td>\n",
       "      <td>20.176441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.606600</td>\n",
       "      <td>18.861065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.949600</td>\n",
       "      <td>18.885086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.125900</td>\n",
       "      <td>18.394253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.034700</td>\n",
       "      <td>18.303366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8000, training_loss=7.761174389839172, metrics={'train_runtime': 630.6547, 'train_samples_per_second': 1623.71, 'train_steps_per_second': 12.685, 'total_flos': 3128934368467344.0, 'train_loss': 7.761174389839172, 'epoch': 72.72727272727273})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoConfig, AutoModelForTokenClassification\n",
    "from knowledge_distillation import DistillationTrainer\n",
    "from dataset import FilteredDataCollatorForTokenClassification, get_tokenized_dataset\n",
    "from metrics import compute_f1, compute_f1_on_dataset\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerBase\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from utils import set_all_seeds, print_model_weights\n",
    "import torch\n",
    "\n",
    "set_all_seeds(812)\n",
    "\n",
    "label_names = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "tokenized_dataset = get_tokenized_dataset()\n",
    "\n",
    "data_collator = FilteredDataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "student_config = AutoConfig.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    num_labels=len(label_names),\n",
    "    hidden_size=384,\n",
    "    num_hidden_layers=8,\n",
    "    num_attention_heads=6,\n",
    "    intermediate_size=512,\n",
    ")\n",
    "student_model = AutoModelForTokenClassification.from_config(student_config)\n",
    "print_model_weights(student_model)\n",
    "\n",
    "teacher_model = AutoModelForTokenClassification.from_pretrained('checkpoint-1500', num_labels=len(label_names))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "teacher_model = teacher_model.eval().to(device)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_student\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    learning_rate=4e-4,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=64,\n",
    "    max_steps=8000,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_student',\n",
    "    logging_steps=50,\n",
    "    warmup_steps=1000,\n",
    "    save_steps=1000,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = DistillationTrainer(\n",
    "    model=student_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    teacher_model=teacher_model, \n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5ca5941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 216/216 [00:01<00:00, 168.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7064369589440302"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_f1_on_dataset(student_model, tokenized_dataset[\"test\"], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b159870d-4dc7-4e60-89b2-bf99c48b8ee9",
   "metadata": {},
   "source": [
    "# –ó–∞–¥–∞–Ω–∏—è –Ω–∞ –≤—ã–±–æ—Ä\n",
    "\n",
    "–ö–∞–∫ –≤—ã –ø–æ–Ω–∏–º–∞–µ—Ç–µ, –µ—Å—Ç—å –µ—â–µ –¥–æ–≤–æ–ª—å–Ω–æ –º–Ω–æ–≥–æ —Ä–∞–∑–Ω—ã—Ö —Å–ø–æ—Å–æ–±–æ–≤ —É–º–µ–Ω—å—à–∏—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å. –í —ç—Ç–æ–π —Å–µ–∫—Ü–∏–∏ –≤–∞–º –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –Ω–∞ –≤—ã–±–æ—Ä. –ó–∞ –∫–∞–∂–¥—É—é –∏–∑ –Ω–∏—Ö –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å —Ä–∞–∑–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞–ª–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏. –£—Å–ø–µ—à–Ω–æ—Å—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –±—É–¥–µ—Ç –æ—Ü–µ–Ω–∏–≤–∞—Ç—å—Å—è –∫–∞–∫ –ø–æ –∫–æ–¥—É, —Ç–∞–∫ –∏ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ. –í—Å–µ –±–∞–ª–ª—ã –∑–∞ —ç—Ç–æ –¥–∑, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –Ω–∞–±–µ—Ä–µ—Ç–µ —Å–≤–µ—Ä—Ö 10, –±—É–¥—É—Ç —Å—á–∏—Ç–∞—Ç—å—Å—è –±–æ–Ω—É—Å–Ω—ã–º–∏.   \n",
    "–í –∑–∞–¥–∞–Ω–∏–∏ 4 –≤—ã –æ–±—É—á–∞–ª–∏ –º–æ–¥–µ–ª—å —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —á–∏—Å–ª–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ 20–ú. –ü—Ä–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ—Ö–Ω–∏–∫ –∏–∑ —ç—Ç–æ–π —Å–µ–∫—Ü–∏–∏ –ø—Ä–∏–¥–µ—Ä–∂–∏–≤–∞–π—Ç–µ—Å—å —Ç–∞–∫–æ–≥–æ –∂–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ. –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç —á–µ—Å—Ç–Ω–æ —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –º–µ—Ç–æ–¥—ã –º–µ–∂–¥—É —Å–æ–±–æ–π –∏ –¥–µ–ª–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –≤—ã–≤–æ–¥—ã. –ù–∞–ø–∏—à–∏—Ç–µ –≤ –æ—Ç—á–µ—Ç–µ –æ–±–æ –≤—Å–µ–º, —á—Ç–æ –≤—ã –ø–æ–ø—Ä–æ–±–æ–≤–∞–ª–∏.\n",
    "\n",
    "* __–®–µ—Ä–∏–Ω–≥ –≤–µ—Å–æ–≤ (2 –±–∞–ª–ª–∞).__ –í –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ BERT [ALBERT](https://arxiv.org/pdf/1909.11942.pdf) –ø–æ–º–∏–º–æ —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —à–µ—Ä–∏—Ç—å –≤–µ—Å–∞ –º–µ–∂–¥—É —Å–ª–æ—è–º–∏. –¢–æ –µ—Å—Ç—å —Ä–∞–∑–Ω—ã–µ —Å–ª–æ–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –æ–¥–Ω–∏ –∏ —Ç–µ –∂–µ –≤–µ—Å–∞. –¢–∞–∫–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ —ç–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ —Å–ª–æ—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —É–º–µ–Ω—å—à–∏—Ç—å —á–∏—Å–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –Ω–µ —Å–∏–ª—å–Ω–æ –ø–æ—Ç–µ—Ä—è—Ç—å –≤ –∫–∞—á–µ—Å—Ç–≤–µ.\n",
    "* __–§–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Å–ª–æ–µ–≤ (2 –±–∞–ª–ª–∞).__ –ï—Å–ª–∏ –º–æ–∂–Ω–æ —Ñ–∞–∫—Ç–æ—Ä–∏–∑–æ–≤–∞—Ç—å –º–∞—Ç—Ä–∏—Ü—É —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, —Ç–æ –∏ –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω–æ–µ —Ç–æ–∂–µ –º–æ–∂–Ω–æ. –î–ª—è —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–µ–≤ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –º–Ω–æ–≥–æ —Ä–∞–∑–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∏ –≤—ã–±—Ä–∞—Ç—å –∫–∞–∫–æ–π-—Ç–æ –æ–¥–∏–Ω —Å–ª–æ–∂–Ω–æ. –í—ã –º–æ–∂–µ—Ç–µ –≤–¥–æ—Ö–Ω–æ–≤–ª—è—Ç—å—Å—è [—ç—Ç–∏–º —Å–ø–∏—Å–∫–æ–º](https://lechnowak.com/posts/neural-network-low-rank-factorization-techniques/), –Ω–∞–π—Ç–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ —á—Ç–æ-—Ç–æ –¥—Ä—É–≥–æ–µ –∏–ª–∏ –ø—Ä–∏–¥—É–º–∞—Ç—å –º–µ—Ç–æ–¥ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ. –í –ª—é–±–æ–º —Å–ª—É—á–∞–µ –≤ –æ—Ç—á–µ—Ç–µ –æ–±–æ—Å–Ω—É–π—Ç–µ, –ø–æ—á–µ–º—É –≤—ã —Ä–µ—à–∏–ª–∏ —Å–¥–µ–ª–∞—Ç—å —Ç–∞–∫ –∫–∞–∫ —Å–¥–µ–ª–∞–ª–∏.\n",
    "* __–ü—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Å–ª–æ–µ–≤ (2 –±–∞–ª–ª–∞).__ –ú—ã –æ–±—Å—É–∂–¥–∞–ª–∏, —á—Ç–æ –ø–æ–º–∏–º–æ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏—è –≤—ã—Ö–æ–¥–æ–≤ –º–æ–¥–µ–ª–∏ —É—á–µ–Ω–∏–∫–∞ –∫ –≤—ã—Ö–æ–¥–∞–º –º–æ–¥–µ–ª–∏ —É—á–∏—Ç–µ–ª—è, –º–æ–∂–Ω–æ –ø—Ä–∏–±–ª–∏–∂–∞—Ç—å –≤—ã—Ö–æ–¥—ã –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Å–ª–æ–µ–≤. –í [—ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ](https://www.researchgate.net/publication/375758425_Knowledge_Distillation_Scheme_for_Named_Entity_Recognition_Model_Based_on_BERT) –ø–æ–¥—Ä–æ–±–Ω–æ –Ω–∞–ø–∏—Å–∞–Ω–æ, –∫–∞–∫ —ç—Ç–æ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å.\n",
    "* __–ü—Ä—É–Ω–∏–Ω–≥ (4 –±–∞–ª–ª–∞).__ –í –º–µ—Ç–æ–¥–µ [SparseGPT](https://arxiv.org/abs/2301.00774) –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –ø–æ–¥—Ö–æ–¥, —É–¥–∞–ª—è—é—â–∏–π –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –æ–¥–∏–Ω —Ä–∞–∑ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è. –ü—Ä–∏ —ç—Ç–æ–º –æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –≤–æ–∑–º–æ–∂–Ω—ã–º —É–¥–∞–ª–∏—Ç—å –¥–æ –ø–æ–ª–æ–≤–∏–Ω—ã –≤—Å–µ—Ö –≤–µ—Å–æ–≤ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ. –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞, —Å—Ç–æ—è—â–∞—è—è –∑–∞ —Ç–µ—Ö–Ω–∏–∫–æ–π, –¥–æ–≤–æ–ª—å–Ω–æ —Å–ª–æ–∂–Ω–∞—è, –æ–¥–Ω–∞–∫–æ –æ–±—â–∏–π –ø–æ–¥—Ö–æ–¥ –ø—Ä–æ—Å—Ç–æ–π ‚Äì¬†–±—É–¥–µ–º —É–¥–∞–ª—è—Ç—å –≤–µ—Å–∞ –≤ –∫–∞–∂–¥–æ–º —Å–ª–æ–µ –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏, –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ —á–∞—Å—Ç–∏ –≤–µ—Å–æ–≤ —Å–ª–æ—è, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ –±—É–¥—É—Ç –ø–µ—Ä–µ–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å—Å—è —Ç–∞–∫, —á—Ç–æ–±—ã –æ–±—â–∏–π –≤—ã—Ö–æ–¥ —Å–ª–æ—è –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è.\n",
    "* __–£–¥–∞–ª–µ–Ω–∏–µ –≥–æ–ª–æ–≤ (6 –±–∞–ª–ª–æ–≤).__ –í –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è, –Ω–æ —Ä—è–¥ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –∏–∑ –Ω–∏—Ö –º–æ–∂–Ω–æ –≤—ã–±—Ä–æ—Å–∏—Ç—å –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞. –í —ç—Ç–æ–π [—Å—Ç–∞—Ç—å–µ](https://arxiv.org/pdf/1905.09418.pdf) –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –ø–æ–¥—Ö–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –¥–æ–±–∞–≤–ª—è–µ—Ç –≥–µ–π—Ç—ã –∫ –º–µ—Ö–∞–Ω–∏–∑–º—É –≤–Ω–∏–º–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–≥—É–ª–∏—Ä—É—é—Ç, –∫–∞–∫–∏–µ –≥–æ–ª–æ–≤—ã —É—á–∞—Å—Ç–≤—É—é—Ç –≤ —Å–ª–æ–µ, –∞ –∫–∞–∫–∏–µ ‚Äì¬†–Ω–µ—Ç. –í –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è –≥–µ–π—Ç—ã –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—é—Ç—Å—è —Ç–∞–∫, —á—Ç–æ–±—ã –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å. –í –∫–æ–Ω—Ü–µ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –≥–æ–ª–æ–≤—ã –º–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å. –ó–∞ —ç—Ç–æ –∑–∞–¥–∞–Ω–∏–µ –¥–∞–µ—Ç—Å—è –º–Ω–æ–≥–æ –±–∞–ª–ª–æ–≤, –ø–æ—Ç–æ–º—É —á—Ç–æ –≤ –º–µ—Ç–æ–¥–µ –¥–æ–≤–æ–ª—å–Ω–æ —Å–ª–æ–∂–Ω–∞—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –∏ –ø–æ–¥—Ö–æ–¥ –ø–ª–æ—Ö–æ –∑–∞–≤–æ–¥–∏—Ç—Å—è. –ï—Å–ª–∏ –≤—ã —Ä–µ—à–∏—Ç–µ—Å—å –ø–æ—Ç—Ä–∞—Ç–∏—Ç—å –Ω–∞ –Ω–µ–≥–æ —Å–≤–æ–∏ —Å–∏–ª—ã, —Ç–æ –≤ —Å–ª—É—á–∞–µ –Ω–µ—É–¥–∞—á–∏ –º—ã –¥–∞–¥–∏–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –±–∞–ª–ª—ã, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ –æ—Ç—á–µ—Ç.   \n",
    "__–°–æ–≤–µ—Ç:__ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ —Å–ª–µ–¥–∏—Ç–µ –∑–∞ –ø–æ–≤–µ–¥–µ–Ω–∏–µ–º –≥–µ–π—Ç–æ–≤. –ï—Å–ª–∏ –≤—ã –≤—Å–µ —Å–¥–µ–ª–∞–ª–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ, —Ç–æ –æ–Ω–∏ –¥–æ–ª–∂–Ω—ã –∑–∞–Ω—É–ª—è—Ç—å—Å—è. –û–¥–Ω–∞–∫–æ –∑–∞–Ω—É–ª—è—é—Ç—Å—è –æ–Ω–∏ –Ω–µ –≤—Å–µ–≥–¥–∞ —Å—Ä–∞–∑—É, –∏–º –Ω–∞–¥–æ –¥–∞—Ç—å –≤—Ä–µ–º—è –∏ –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å –ø–æ–¥–æ–ª—å—à–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a327cc9",
   "metadata": {},
   "source": [
    "## –®—ç—Ä–∏–Ω–≥ –≤–µ—Å–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bb28dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 19517065\n",
      "Trainable Parameters: 19517065\n",
      "Non-Trainable Parameters: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/make_bert_small_again/knowledge_distillation.py:27: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `DistillationTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8000' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8000/8000 51:35, Epoch 72/73]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>15.106100</td>\n",
       "      <td>22.646357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>5.920400</td>\n",
       "      <td>20.382185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.872000</td>\n",
       "      <td>19.348129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.805400</td>\n",
       "      <td>20.201607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.571000</td>\n",
       "      <td>18.843483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.856600</td>\n",
       "      <td>18.927162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.019200</td>\n",
       "      <td>18.228374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.929700</td>\n",
       "      <td>18.020826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8000, training_loss=8.27344127893448, metrics={'train_runtime': 3095.9528, 'train_samples_per_second': 330.754, 'train_steps_per_second': 2.584, 'total_flos': 997777678377360.0, 'train_loss': 8.27344127893448, 'epoch': 72.72727272727273})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoConfig, AutoModelForTokenClassification\n",
    "from knowledge_distillation import DistillationTrainer\n",
    "from dataset import FilteredDataCollatorForTokenClassification, get_tokenized_dataset\n",
    "from metrics import compute_f1, compute_f1_on_dataset\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerBase\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from utils import set_all_seeds, print_model_weights\n",
    "from weights_sharing import BertSharedForTokenClassification\n",
    "import torch\n",
    "\n",
    "set_all_seeds(1488)\n",
    "\n",
    "label_names = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "tokenized_dataset = get_tokenized_dataset()\n",
    "\n",
    "data_collator = FilteredDataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "student_config = AutoConfig.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    num_labels=len(label_names),\n",
    "    hidden_size=576,\n",
    "    num_hidden_layers=18,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=1024,\n",
    ")\n",
    "\n",
    "student_model = BertSharedForTokenClassification(student_config)\n",
    "print_model_weights(student_model)\n",
    "\n",
    "teacher_model = AutoModelForTokenClassification.from_pretrained('results_vanilla/checkpoint-2000', num_labels=len(label_names))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "teacher_model = teacher_model.eval().to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_student_weights_sharing_18_layers\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=64,\n",
    "    max_steps=8000,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_student',\n",
    "    logging_steps=50,\n",
    "    warmup_steps=1000,\n",
    "    save_steps=1000,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = DistillationTrainer(\n",
    "    model=student_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    teacher_model=teacher_model, \n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81c0c535-ebca-4c6e-a15e-dbb103f42168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 216/216 [00:02<00:00, 91.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7281655559452823"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_f1_on_dataset(student_model, tokenized_dataset[\"test\"], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fefee07",
   "metadata": {},
   "source": [
    "## –§–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Å–ª–æ–µ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "570dbb7c-b691-4e68-a655-0c53aee7b9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Linear layer classifier with dimensions 768x9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_6162/2431375302.py:50: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 19892565\n",
      "Trainable Parameters: 19892565\n",
      "Non-Trainable Parameters: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2056' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2056/3000 10:28 < 04:48, 3.27 it/s, Epoch 18.68/28]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.142120</td>\n",
       "      <td>0.819126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.079400</td>\n",
       "      <td>0.112095</td>\n",
       "      <td>0.860701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.124072</td>\n",
       "      <td>0.872977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.117952</td>\n",
       "      <td>0.885623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>0.135960</td>\n",
       "      <td>0.885385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>0.123772</td>\n",
       "      <td>0.894161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.134250</td>\n",
       "      <td>0.895110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.146382</td>\n",
       "      <td>0.901791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.142285</td>\n",
       "      <td>0.899636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.154918</td>\n",
       "      <td>0.900605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 60\u001b[0m\n\u001b[1;32m     34\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     35\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results_all_factorized\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     36\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     50\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     51\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_factorized,\n\u001b[1;32m     52\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_f1\n\u001b[1;32m     58\u001b[0m )\n\u001b[0;32m---> 60\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3612\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3610\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3612\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3613\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[1;32m   3614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2241\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from linear_factorized import replace_linear_with_factorized\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from dataset import FilteredDataCollatorForTokenClassification, get_tokenized_dataset\n",
    "from metrics import compute_f1, compute_f1_on_dataset\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerBase\n",
    "from factorized_embeddings import initialize_with_svd\n",
    "from utils import print_model_weights, set_all_seeds\n",
    "import torch\n",
    "\n",
    "set_all_seeds(1488)\n",
    "\n",
    "label_names = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "tokenized_dataset = get_tokenized_dataset()\n",
    "\n",
    "data_collator = FilteredDataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "model_factorized = AutoModelForTokenClassification.from_pretrained('results_vanilla/checkpoint-2000', num_labels=len(label_names))\n",
    "\n",
    "hidden_dim = 99\n",
    "replace_linear_with_factorized(model_factorized, hidden_dim)\n",
    "\n",
    "original_embedding = model_factorized.bert.embeddings.word_embeddings\n",
    "factorized_embedding = initialize_with_svd(original_embedding, hidden_dim)\n",
    "\n",
    "model_factorized.bert.embeddings.word_embeddings = factorized_embedding\n",
    "\n",
    "print_model_weights(model_factorized)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_all_factorized\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=64,\n",
    "    max_steps=3000,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_student',\n",
    "    logging_steps=50,\n",
    "    warmup_steps=500,\n",
    "    save_steps=500,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_factorized,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_f1\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80921d4e-07d8-47d6-bee8-e0a89bbd9a72",
   "metadata": {},
   "source": [
    "–ú–Ω–µ –∫–∞–∂–µ—Ç—Å—è —É–∂–µ —Ö–≤–∞—Ç–∏—Ç, –≤—Ä–æ–¥–µ –Ω–æ—Ä–º –ø–æ—É—á–∏–ª–∏. –î–æ—É—á–∏–≤–∞—Ç—å –Ω–µ —Ö–æ—á–µ—Ç—Å—è -- —Ç–∞–º —É–∂–µ –ª–æ—Å—Å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å–∏–ª—å–Ω–æ —Ä–∞—Å—Ç–µ—Ç. –ò –Ω–∞ —Ç–µ—Å—Ç–µ –∫—Ä—É—Ç–æ –ø–æ–ª—É—á–∞–µ—Ç—Å—è, —Å–º–æ—Ç—Ä–∏–º:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00723c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 216/216 [00:02<00:00, 76.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8398551973797621"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_f1_on_dataset(model_factorized, tokenized_dataset[\"test\"], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23640457-cab2-4a41-88b3-c90b0e025404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
