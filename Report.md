# Отчет

Я не стал делать WandB отчет и не стал рисовать там графики, потому что все эксперименты были очень быстрые и я буквально совсем чуть-чуть менял гиперпараметры всего пару раз. Поэтому я подумал, что будет гораздо удобнее, когда все в одном ноутбуке, а не очень много маленьких и непонятных ранов.

Но! Я, конечно, не стал городить все в ноутбуке и разделил все на файлы.

В каждом разделе ноутбука, где надо провести эксперимент есть большая ячейка. Эта ячейка самодостаточная и не зависит от других. В ее output пишется loss и после нее есть еще одна ячейка с замером f1.

Пройдемся по порядку по всем ячейкам&

## Модель

Тут все понятно: просто скачали и начали тренировать Trainer из HF. Совсем чуть-чуть поперебирал параметры и хватило 2000 итераций с 128 размером батча, чтобы выбить 0.9. Еще 1500 оказался неплохим чекпоинтом и где-то я использовал его в последующих экспах.

## Факторизация матрицы эмбеддингов

Тут тоже ничего интересного: написал класс-обертку над матрицей эмбедов. Он лежит в файле `factorized_embeddings.py`. Просто взял обычную модель и прям руками заменил слой эмбеддингов в ней на факторизованный. При этом инициализировал его с помощью SVD от большого слоя. Все как и написано в задании.

Тренировка тоже дефолтная: хватило 1500 шагов с 128 батч сайзом, чтобы выбить > 0.87. Остальные параметры почти не изменились по сравнению с пред. экспом.

## Дистилляция знаний

Написал кастомную функцию ошибки, которая лежит в `knowledge_distillation.py`. Там же лежит и кастомный `Trainer`, который работает с этой функцией потерь. Функция потерь ровно такая, как описана в задании. Только еще alpha добавлен, чтобы суммировать лоссы (см. в файле).

С тренировкой тут немного хитрее: пришлось увеличить lr, добавить warmup на 1000 шагов и учить все 8к итераций с размером батча 128.

## Шэринг весов 

Тут вообще все супер-просто оказалось. Я взял класс энкодера Берта из hf и убрал все слои, оставив лишь один. И применял его столько раз, сколько надо (ну ровно как в статье). Код для этого можно найти в `weights_sharing.py`. 

Училось все это дело с помощью дистиляции, параметры +- такие же, как и в случае предыдущего задания удалось выбить 0.7281655559452823

## Факторизация промежуточных слоев

Для этого задания был написан класс, который может заменять `nn.Linear`, он лежит в `linear_factorized.py`. Его можно инициализировать с помощью обычного `nn.Linear`, он разложит матрицу весов в нем на две с помощью SVD и будем делать forward, перемножая их.

Далее в цикле я заменил все линейные слои берта с чекпоинта из задания 1 на факторизованные и потом еще добавил факторизацию эмбеддингов из задания 2.

Тренировал я без дистиляции, добавил немного warmup и всего учил 3000 итераций с 128 батч сайз. 
Удалось выбить аж 0.8398551973797621! Что мне кажется супер круто с учетом того, что модель сильно меньше оригинальной.

Я также попробовал тренировать с дистиляцией, но это не дало хорошего эффекта (получалось в районе 0.7). Мне кажется, потому что мы когда факторизуем, то у нас модель все-таки не совсем нулевая, а там довольно много знаний еще есть, поэтому нет смысла дистиллить и надо именно учить.


## 

Вот и все!

Если что -- все параметры тренировки написаны в ноутбуке, все ячейки должны быть runnab;e по отдельности и если надо, то я могу скинуть все чекпоинты. У меня все пруфы сохранены! 